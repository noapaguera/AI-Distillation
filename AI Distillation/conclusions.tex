\ifcase\doclanguage\or
\chapter{Conclusions i Línies Futures}
  \section{Conclusions}
    \begin{itemize}
      \item Resumiu els resultats principals del vostre treball.
      \item Discutiu el grau d'assoliment en relació amb els objectius marcats a l'inici del treball.
      \item Destaqueu les contribucions del vostre treball al camp d'estudi.
    \end{itemize}
  
  \section{Línies Futures}
    \begin{itemize}
      \item Identifiqueu àrees per a futures investigacions o desenvolupament basades en el vostre treball.
      \item Discutiu possibles vies per ampliar o millorar el projecte.
      \item Considereu les preguntes que han quedat sense resposta i les oportunitats per a futures exploracions.
    \end{itemize}

\else
\chapter{Conclusions and Future Work}

    The proposed pipeline demonstrates that the integration of distillation, low-rank adaptation, and quantization provides a robust framework for model compression. The findings confirm that it is possible to achieve a reduction in memory usage compared to the teacher model while maintaining competitive accuracy levels. This makes the resulting model suitable for resource-constrained environments where the original baseline would be computationally infeasible.
    
    Currently, the compression stages were executed with fixed parameters: a specific rank r for LoRA and a specific bit-width for quantization. Future research could investigate how to automatically find the optimal balance between distillation temperature and quantization thresholds.
    
    A further line of research is to frame the distillation process within an information-theoretic context and explore extending the pipeline to a Federated Learning setting for decentralized, privacy-preserving training.
    
    While this thesis focused on theoretical model size and accuracy, the pipeline could be extended by testing performance on diverse edge devices such as Raspberry Pi or mobile CPUs. This would provide empirical data on "Inference latency vs. power consumption" that goes beyond static model size.

\fi