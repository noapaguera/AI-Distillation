%%%% PLEASE REPLACE ENTIRELY WITH YOUR OWN CONTENT %%%%
\ifcase\doclanguage\or
  \chapter{Resultats}
  Aquest capítol ha d'incloure l'anàlisi de les vostres dades i els resultats obtinguts. A més, incloeu-hi taules, figures i citacions pertinents per donar suport als vostres resultats i interpretacions. Aquí teniu una llista suggerida de temes a tractar:
  
    \section{Experiments i proves}
    Descriviu els experiments realitzats per provar el rendiment del vostre projecte. Expliqueu com heu recopilat i processat les dades.
    
    \section{Visualització de les dades}
    Creeu representacions visuals dels resultats (per exemple, gràfics de dispersió, diagrames de barres). Interpreteu les visualitzacions i relacioneu-les amb les preguntes de recerca.
    
    \section{Limitacions}
    Reconeixeu qualsevol limitació en les dades o l'anàlisi. Expliqueu com aquestes limitacions podrien haver afectat els resultats.
  
\else
  \chapter{Results}
  
    \section{Experiments and Tests}
    
    % Describe the experiments conducted to assess the performance of your project. Explain how you collected and processed the data.
    
    This chapter provides an overview of the experimental setup used for training and evaluating the model. It specifically addresses the configurations implemented to achieve the results discussed. For further methodological details or prerequisites, please refer to Chapter 3. 
	In this section, the parameters employed for fine-tuning are defined.
	  
	  \textbf{Fine-tuning Configuration}
	  
	  Fine-tuning of the MobileNet model is performed using a custom configuration that defines the parameters used in training, optimization, and overall model behavior. This setup ensures the model can adapt to the specific dataset used in our experiments. The configuration remains consistent across all trained models to maintain uniformity.
	  
	  The model uses a 32-bit precision format (FP32), a storage format for floating-point values suitable for applications where higher precision is not critical. This is particularly advantageous in scenarios like image processing and neural network training, where computational efficiency and memory optimization are crucial.
	  
	  Fixed random seed values of 42, 518, 1993 are used to ensure reproducibility, enabling consistent results across multiple runs.
	  
	  To maintain efficiency during training, only the highest-performing checkpoint is retained. Selection of the best checkpoint is based on the validation loss metric, ensuring that the stored model reflects peak performance for future evaluation.
	  
	  The model is pre-trained on the ImageNet dataset for image classification. Data pre-processing includes normalization to standardize inputs before they are fed into the model. %	Image augmentation is also applied, introducing random transformations to increase robustness to data variability.
	  
	  For optimization, we use the Adam optimizer, a widely adopted choice due to its adaptability to changing gradients during training. The model begins training with an initial learning rate of 0.001, chosen to provide stable updates without causing divergence.
	   
	  % quantization works well but performance suffers from accuracy loss during inference
	  
	  % doing calculation in int8 offers some advantages
	  % faster arithmetic, lower memory demands reduced by 75%, reduced resource requirements. 
	  
	  Knowledge Distillation Results
	  
	  The implementation of the knowledge distillation phase aimed to transfer the predictions of the ResNet18 to the more compact MobileNetV2. Initial results indicate that the student model achieved an accuracy of 62\%, successfully recovering 76\% of the teacher's performance. This suggests that the soft targets provided by the teacher were effective in regularizing the student's training, despite the significant reduction in total parameter count.
	  
	  LoRA \& Quantization Trade-offs
	  
	  To further optimize the model for deployment, LoRA fine-tuning was applied with 8-bit quantization. While quantization typically introduces a quantization error that degrades performance, the LoRA adapters effectively compensated for this loss. The final quantized model reached a compressed size of 79\% compared to the baseline, with an accuracy degradation of only 12\%.
	  
	  The proposed pipeline demonstrates that the integration of distillation, low-rank adaptation, and quantization provides a robust framework for model compression. The findings confirm that it is possible to achieve a reduction in memory usage while maintaining competitive accuracy levels. This makes the resulting model suitable for resource-constrained environments where the original baseline would be computationally infeasible.
	  
	  %\textbf{Evaluation Framework}
    
    \section{Data Visualization}
    Create visual representations of the results (e.g., scatter plots, bar charts). Interpret the visualizations and relate them to the research questions.
    
    \section{Discussion}
    Acknowledge any limitations in the data or analysis. Explain how these limitations may have influenced the results.

\fi

%
%%%% SECTION3 %%%
%\newpage
%%\vspace*{2cm}
%\section{Section 3}
%\label{sec:sec3}
%
%\lipsum[4] \ac{EU} is the European Union. \lipsum[5]
%\lipsum[6] \ac{ETSETB} is Telecos. \lipsum[7]
%
%\subsection{Subsection}
%\label{sec:subsec3.1}
%The book \cite{latexcompanion} \lipsum[15]
%
%\include{codi_tempdistrib}
%
%%%% SECTION4 %%%
%\input{section4}
%
%%%% SECTION5 %%%
%\newpage
%%\vspace*{2cm}
%\section{Section 5}´
%\label{sec:sect5}
%\lipsum[4]
%
%\subsection{Overview}
%\label{subsec:sect5Overview}
%\lipsum[10]
%Visite the Knuth repository \cite{knuthwebsite}.
%
%%%% TESTING %%%
%\clearpage
%%\vspace*{2cm}
%\section{Experiments and results}
%\label{sec:tests}
%\lipsum[9]
%
