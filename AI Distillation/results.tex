%%%% PLEASE REPLACE ENTIRELY WITH YOUR OWN CONTENT %%%%
\ifcase\doclanguage\or
  \chapter{Resultats}
  Aquest capítol ha d'incloure l'anàlisi de les vostres dades i els resultats obtinguts. A més, incloeu-hi taules, figures i citacions pertinents per donar suport als vostres resultats i interpretacions. Aquí teniu una llista suggerida de temes a tractar:
  
    \section{Experiments i proves}
    Descriviu els experiments realitzats per provar el rendiment del vostre projecte. Expliqueu com heu recopilat i processat les dades.
    
    \section{Visualització de les dades}
    Creeu representacions visuals dels resultats (per exemple, gràfics de dispersió, diagrames de barres). Interpreteu les visualitzacions i relacioneu-les amb les preguntes de recerca.
    
    \section{Limitacions}
    Reconeixeu qualsevol limitació en les dades o l'anàlisi. Expliqueu com aquestes limitacions podrien haver afectat els resultats.
  
\else
  \chapter{Results}
  
    \section{Experiments and Tests}
    
    This chapter provides an overview of the experimental setup used for training and evaluating the model. It specifically addresses the configurations implemented to achieve the results discussed. For further methodological details or prerequisites, please refer to Chapter 3. 
	In this section, the parameters employed for fine-tuning are defined.
	  
	  \textbf{Fine-tuning Configuration}
	  
	  Fine-tuning of the MobileNet model is performed using a custom configuration that defines the parameters used in training, optimization, and overall model behavior. This setup ensures the model can adapt to the specific dataset used in our experiments. The configuration remains consistent across all trained models to maintain uniformity.
	  
	  The model uses a 32-bit precision format (FP32), a storage format for floating-point values suitable for applications where higher precision is not critical. This is particularly advantageous in scenarios like image processing and neural network training, where computational efficiency and memory optimization are crucial.
	  
	  Fixed random seed values of 42, 518, 1993 are used to ensure reproducibility, enabling consistent results across multiple runs.
	  
	  To maintain efficiency during training, only the highest-performing checkpoint is retained. Selection of the best checkpoint is based on the validation loss metric, ensuring that the stored model reflects peak performance for future evaluation.
	  
	  The model is pre-trained on the ImageNet dataset for image classification. Data pre-processing includes normalization to standardize inputs before they are fed into the model. Image augmentation is also applied, introducing random transformations to increase robustness to data variability.
	  
	  For optimization, we use the Adam optimizer, a widely adopted choice due to its adaptability to changing gradients during training. The model begins training with an initial learning rate of 0.001, chosen to provide stable updates without causing divergence.	% quantization works well but performance suffers from accuracy loss during inference % doing calculation in int8 offers some advantages % faster arithmetic, lower memory demands reduced by 75%, reduced resource requirements.	  
	      
    \section{Data Visualization}
    
    Figure 4.1 illustrates the baseline accuracy of the student model. These results serve as a primary benchmark for comparison against the distilled model, which is trained using the soft outputs of the teacher model.
    
    \begin{figure}[h!]
    	\centering
    	\includegraphics[width=1\linewidth]{img/student_baseline_v1.png}
    	\caption{Baseline MobileNetV2 student model}
    	\label{fig:placeholder}
    \end{figure}

    \textbf{Knowledge Distillation Results}
    
    The implementation of the knowledge distillation phase aimed to transfer the predictions of the ResNet18 to the more compact MobileNetV2. The results in Figure \ref{fig:distil} indicate that the student model achieved an accuracy of 79.2\%, successfully recovering 97\% of the teacher's performance. This suggests that the soft targets provided by the teacher were effective in regularizing the student's training, despite the significant reduction in total parameter count.
    
    \begin{figure}[h!]
    	\centering
    	\includegraphics[width=1\linewidth]{img/KD_v1.png}
    	\caption{MobileNetV2 student model after distillation}
    	\label{fig:distil}
    \end{figure}
       
    \textbf{LoRA \& Quantization Trade-offs}
    
    To further optimize the model for deployment, LoRA fine-tuning was applied with 8-bit quantization. While quantization typically introduces a quantization error that degrades performance, the LoRA adapters effectively compensated for this loss. The final quantized model reached a compressed size of 79\% compared to the baseline, with a minimum accuracy degradation.% of only 12\%.
       
    \begin{figure}[h!]
		\centering
		\includegraphics[width=1\linewidth]{img/lora_v2.png}
		\caption{MobileNetV2 student model after LoRA Fine-Tuning}
		\label{fig:lora-plot}
	\end{figure}
    
    \section{Discussion}
    
    In summary, this research successfully established a pipeline for model compression. However, several limitations must be acknowledged. First, the reliance on a fixed, balanced dataset, while providing a controlled environment for testing, may mask the model's performance in real-world, imbalanced scenarios.
    
    Secondly, the observed inconsistencies in the LoRA fine-tuning phase suggest a sensitivity to hyperparameter settings that was not fully captured within the current training volume. The inclusion of synthetic data generation could address these variances in future iterations.
    
    Despite these constraints, the results demonstrate that the proposed pipeline achieves >75\% student model accuracy, providing a viable framework for further optimization in image classification.
    
    \begin{table}[h!]
    	\begin{tabular}{|l|l|l|l|l|}
    		\hline
    		Model stage                  & Accuracy (\%) & Size (MB) & Inference (ms) & FLOPS   \\ \hline
    		Teacher                      & 80.5          & 44.8      & 95.1           & 74.440M \\ \hline
    		Student without distillation & 76.4          & 9.19      & 66.2           & 13.340M \\ \hline
    		Distilled Student            & 79.0          & 9.19      & 66.2           & 13.340M \\ \hline
    		LoRA-tuned Student           & 79.3          & 9.22      & 77.2           & 13.340M \\ \hline
    		Quantized Student            & 79.5          & 9.18      & 15.1           & 13.340M \\ \hline
    	\end{tabular}
    	\caption{Summary table comparing models' metrics}
    \end{table}
    

\fi

%
%%%% SECTION3 %%%
%\newpage
%%\vspace*{2cm}
%\section{Section 3}
%\label{sec:sec3}
%
%\lipsum[4] \ac{EU} is the European Union. \lipsum[5]
%\lipsum[6] \ac{ETSETB} is Telecos. \lipsum[7]
%
%\subsection{Subsection}
%\label{sec:subsec3.1}
%The book \cite{latexcompanion} \lipsum[15]
%
%\include{codi_tempdistrib}
%
%%%% SECTION4 %%%
%\input{section4}
%
%%%% SECTION5 %%%
%\newpage
%%\vspace*{2cm}
%\section{Section 5}´
%\label{sec:sect5}
%\lipsum[4]
%
%\subsection{Overview}
%\label{subsec:sect5Overview}
%\lipsum[10]
%Visite the Knuth repository \cite{knuthwebsite}.
%
%%%% TESTING %%%
%\clearpage
%%\vspace*{2cm}
%\section{Experiments and results}
%\label{sec:tests}
%\lipsum[9]
%
