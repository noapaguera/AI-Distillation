% old references in /home/noa/Documents/TGZ
@article{hinton2015distillingknowledgeneuralnetwork,
	title={Distilling the Knowledge in a Neural Network}, 
	author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
	year={2015},
	eprint={1503.02531},
	archivePrefix={arXiv},
	primaryClass={stat.ML},
	url={https://arxiv.org/abs/1503.02531}, 
}

@misc{zhang2025parameterefficientfinetuningfoundationmodels,
	title={Parameter-Efficient Fine-Tuning for Foundation Models}, 
	author={Dan Zhang and Tao Feng and Lilong Xue and Yuandong Wang and Yuxiao Dong and Jie Tang},
	year={2025},
	eprint={2501.13787},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2501.13787}, 
}

@misc{hu2021loralowrankadaptationlarge,
	title={LoRA: Low-Rank Adaptation of Large Language Models}, 
	author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	year={2021},
	eprint={2106.09685},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2106.09685}, 
}

%4
@misc{dettmers2023qloraefficientfinetuningquantized,
	title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
	author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
	year={2023},
	eprint={2305.14314},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2305.14314}, 
}

@inproceedings{moslem-2025-efficient,
	title={Efficient Speech Translation through Model Compression and Knowledge Distillation},
	url={http://dx.doi.org/10.18653/v1/2025.iwslt-1.40},
	DOI={10.18653/v1/2025.iwslt-1.40},
	booktitle={Proceedings of the 22nd International Conference on Spoken Language Translation (IWSLT 2025)},
	publisher={Association for Computational Linguistics},
	author={Moslem, Yasmin},
	year={2025},
	pages={379â€“388} }

%6
@misc{wu2024knowledgedistillationfederatededge,
	title={Knowledge Distillation in Federated Edge Learning: A Survey}, 
	author={Zhiyuan Wu and Sheng Sun and Yuwei Wang and Min Liu and Xuefeng Jiang and Runhan Li and Bo Gao},
	year={2024},
	eprint={2301.05849},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2301.05849}, 
}

%7
@misc{wang2022efficientknowledgedistillationmodel,
	title={Efficient Knowledge Distillation from Model Checkpoints}, 
	author={Chaofei Wang and Qisen Yang and Rui Huang and Shiji Song and Gao Huang},
	year={2022},
	eprint={2210.06458},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2210.06458}, 
}

@misc{he2015deepresiduallearningimage,
	title={Deep Residual Learning for Image Recognition}, 
	author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year={2015},
	eprint={1512.03385},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1512.03385}, 
}

@misc{sandler2019mobilenetv2invertedresidualslinear,
	title={MobileNetV2: Inverted Residuals and Linear Bottlenecks}, 
	author={Mark Sandler and Andrew Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
	year={2019},
	eprint={1801.04381},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/1801.04381}, 
}

@misc{cho2019efficacyknowledgedistillation,
	title={On the Efficacy of Knowledge Distillation}, 
	author={Jang Hyun Cho and Bharath Hariharan},
	year={2019},
	eprint={1910.01348},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1910.01348}, 
}

@misc{gandhi2023distilwhisperrobustknowledgedistillation,
	title={Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling}, 
	author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},
	year={2023},
	eprint={2311.00430},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2311.00430}, 
}

@misc{huang2022knowledgedistillationstrongerteacher,
	title={Knowledge Distillation from A Stronger Teacher}, 
	author={Tao Huang and Shan You and Fei Wang and Chen Qian and Chang Xu},
	year={2022},
	eprint={2205.10536},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2205.10536}, 
}
